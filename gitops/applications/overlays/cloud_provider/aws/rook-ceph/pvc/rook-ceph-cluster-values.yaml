# Installs a debugging toolbox deployment
toolbox:
  enabled: true

monitoring:
  # Monitoring requires Prometheus to be pre-installed
  enabled: true
  createPrometheusRules: false
  interval: 60s

cephClusterSpec:
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  removeOSDsIfOutAndSafeToRemove: true

  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
        resources:
          requests:
            storage: "${ARGOCD_ENV_ceph_mon_volume_size}"
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true

  # Ceph dashboard
  dashboard:
    enabled: true
    prometheusEndpoint: http://prometheus-operated.monitoring.svc.cluster.local:9090

  storage:
    allowDeviceClassUpdate: false # whether to allow changing the device class of an OSD after it is created
    allowOsdCrushWeightUpdate: true # whether to allow resizing the OSD crush weight after osd pvc is increased
    storageClassDeviceSets:
    - name: ebs-set1
      # The number of OSDs to create from this device set
      count: ${ARGOCD_ENV_ceph_osd_count}
      portable: true
      encrypted: false
      tuneDeviceClass: true
      tuneFastDeviceClass: false
      # Topology spread constraints
      placement:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: topology.kubernetes.io/zone
                    operator: Exists
        topologySpreadConstraints:
          - maxSkew: 1
            #topologyKey: kubernetes.io/hostname
            #whenUnsatisfiable:  ScheduleAnyway
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - rook-ceph-osd
      preparePlacement:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: topology.kubernetes.io/zone
                    operator: Exists
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd-prepare
                topologyKey: kubernetes.io/hostname
        topologySpreadConstraints:
          - maxSkew: 1
            #topologyKey: kubernetes.io/hostname
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - rook-ceph-osd-prepare
      # OSD daemon limits
      # resources:
      #   limits:
      #     memory: "4Gi"
      #   requests:
      #     cpu: "500m"
      #     memory: "4Gi"
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: "${ARGOCD_ENV_ceph_volume_size_per_osd}"
          storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
    # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement.
    onlyApplyOSDPlacement: false
  resources:
  #  prepareosd:
  #    requests:
  #      cpu: "200m"
  #      memory: "200Mi"
  # priority classes to apply to ceph resource
  priorityClassNames:
    # If there are multiple nodes available in a failure domain (e.g. zones), the
    # mons and osds can be portable and set the system-cluster-critical priority class.
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

# CephObjectStore configurations
cephObjectStores:
  - name: ceph-objectstore
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: host
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
      preservePoolsOnDelete: true
      gateway:
        port: 80
        # securePort: 443
        instances: 1
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      parameters:
        # note: objectStoreNamespace and objectStoreName are configured by the chart
        region: "${ARGOCD_ENV_rook_ceph_object_store_region}"
    ingress:
      # Enable an ingress for the ceph-objectstore
      enabled: false

# -- A list of CephBlockPool configurations to deploy
# @default -- See [below](#ceph-block-pools)
cephBlockPools:
  - name: ceph-blockpool
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
    spec:
      failureDomain: host
      replicated:
        size: 3
      # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
      # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
      # enableRBDStats: true
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        imageFormat: "2"
        # These secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "${ARGOCD_ENV_rook_ceph_namespace}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "${ARGOCD_ENV_rook_ceph_namespace}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "${ARGOCD_ENV_rook_ceph_namespace}"
        csi.storage.k8s.io/fstype: ext4

# -- Settings for the filesystem snapshot class
cephFileSystemVolumeSnapshotClass:
  enabled: false
  name: ceph-filesystem
  isDefault: true
  deletionPolicy: Delete

# -- Settings for the block pool snapshot class
cephBlockPoolsVolumeSnapshotClass:
  enabled: false
  name: ceph-block
  isDefault: false
  deletionPolicy: Delete