# Installs a debugging toolbox deployment
toolbox:
  enabled: true

monitoring:
  # Monitoring requires Prometheus to be pre-installed
  enabled: true
  createPrometheusRules: false
  interval: 60s

cephClusterSpec:
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: true

  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
        resources:
          requests:
            storage: "${ARGOCD_ENV_ceph_mon_volume_size}"
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true

  # enable the ceph dashboard
  dashboard:
    enabled: true
    prometheusEndpoint: http://prometheus-operated.monitoring.svc.cluster.local:9090

  storage:
    allowDeviceClassUpdate: false # whether to allow changing the device class of an OSD after it is created
    allowOsdCrushWeightUpdate: true # whether to allow resizing the OSD crush weight after osd pvc is increased
    storageClassDeviceSets:
    - name: set1
      # The number of OSDs to create from this device set
      count: ${ARGOCD_ENV_ceph_osd_count}
      portable: false
      encrypted: false
      tuneDeviceClass: true
      tuneFastDeviceClass: false
      # Topology spread constraints
      placement:
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            #topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - rook-ceph-osd
      preparePlacement:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd-prepare
                topologyKey: kubernetes.io/hostname
        topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              #topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd-prepare
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: "${ARGOCD_ENV_ceph_volume_size_per_osd}"
          storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
    onlyApplyOSDPlacement: false
  priorityClassNames:
    # If there are multiple nodes available in a failure domain (e.g. zones), the
    # mons and osds can be portable and set the system-cluster-critical priority class.
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

# CephObjectStore configurations
cephObjectStores:
  - name: ceph-objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: host
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      parameters:
        region: "${ARGOCD_ENV_rook_ceph_object_store_region}"
    ingress:
      enabled: false

# Ceph Block Pools configurations
cephBlockPools:
  - name: ceph-blockpool
    spec:
      failureDomain: host #host, osd
      replicated:
        size: 3
    storageClass:
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        csi.storage.k8s.io/fstype: ext4