# Values for a single rook-ceph cluster

# Installs a debugging toolbox deployment
toolbox:
  enabled: true

monitoring:
  # Monitoring requires Prometheus to be pre-installed
  enabled: true
  createPrometheusRules: false
  interval: 60s

cephClusterSpec:
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false

  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
        resources:
          requests:
            storage: "${ARGOCD_ENV_ceph_mon_volume_size}"

  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true

  # enable the ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
    prometheusEndpoint: http://prometheus-operated.monitoring.svc.cluster.local:9090

  storage:
    allowDeviceClassUpdate: false # whether to allow changing the device class of an OSD after it is created
    allowOsdCrushWeightUpdate: true # whether to allow resizing the OSD crush weight after osd pvc is increased
    storageClassDeviceSets:
    - name: set1
      # The number of OSDs to create from this device set
      count: ${ARGOCD_ENV_ceph_osd_count}
      # IMPORTANT: If volumes specified by the storageClassName are not portable across nodes
      # this needs to be set to false. For example, if using the local storage provisioner
      # this should be false.
      portable: true
      #failureDomain: zone #new
      # whether to encrypt the deviceSet or not
      encrypted: false
      # Certain storage class in the Cloud are slow
      # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
      # Currently, "gp2-csi" has been identified as such
      tuneDeviceClass: true
      # Certain storage class in the Cloud are fast
      # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
      # Currently, "managed-premium" has been identified as such
      tuneFastDeviceClass: false
      # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
      # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
      # as soon as you have more than one OSD per node. The topology spread constraints will
      # give us an even spread on K8s 1.18 or newer.
      placement:
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            #topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - rook-ceph-osd
      preparePlacement:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd-prepare
                topologyKey: kubernetes.io/hostname
        topologySpreadConstraints:
            - maxSkew: 1
              # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
              topologyKey: topology.kubernetes.io/zone
              #topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd-prepare
      volumeClaimTemplates:
      - metadata:
          name: data
          # set a different CRUSH device class on the OSD than the one detected by Ceph
          # annotations:
          #   crushDeviceClass: hybrid
        spec:
          resources:
            requests:
              storage: "${ARGOCD_ENV_ceph_volume_size_per_osd}"
          # IMPORTANT: Change the storage class depending on your environment
          storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
    # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement.
    onlyApplyOSDPlacement: false
  priorityClassNames:
    # If there are multiple nodes available in a failure domain (e.g. zones), the
    # mons and osds can be portable and set the system-cluster-critical priority class.
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

# -- A list of CephObjectStore configurations to deploy
# @default -- See [below](#ceph-object-stores)
cephObjectStores:
  - name: ceph-objectstore
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
    spec:
      metadataPool:
        replicated:
          size: 3
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
      parameters:
        # note: objectStoreNamespace and objectStoreName are configured by the chart
        region: "${ARGOCD_ENV_rook_ceph_object_store_region}"
    ingress:
      # Enable an ingress for the ceph-objectstore
      enabled: false