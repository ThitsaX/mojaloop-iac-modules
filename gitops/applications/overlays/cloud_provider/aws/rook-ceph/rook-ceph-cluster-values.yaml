# Installs a debugging toolbox deployment
toolbox:
  enabled: true

monitoring:
  # Monitoring requires Prometheus to be pre-installed
  enabled: true
  createPrometheusRules: false
  interval: 60s

cephClusterSpec:
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  removeOSDsIfOutAndSafeToRemove: true

  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
        resources:
          requests:
            storage: "${ARGOCD_ENV_ceph_mon_volume_size}"
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: rook
        enabled: true

  # Ceph dashboard
  dashboard:
    enabled: true
    prometheusEndpoint: http://prometheus-operated.monitoring.svc.cluster.local:9090

  storage:
    allowDeviceClassUpdate: false # whether to allow changing the device class of an OSD after it is created
    allowOsdCrushWeightUpdate: true # whether to allow resizing the OSD crush weight after osd pvc is increased
    storageClassDeviceSets:
    - name: set1
      # The number of OSDs to create from this device set
      count: ${ARGOCD_ENV_ceph_osd_count}
      portable: true
      encrypted: false
      tuneDeviceClass: true
      tuneFastDeviceClass: false
      # Topology spread constraints
      placement:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: topology.kubernetes.io/zone
                    operator: Exists
          topologySpreadConstraints:
            - maxSkew: 1
              #topologyKey: kubernetes.io/hostname
              #whenUnsatisfiable: DoNotSchedule
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
        preparePlacement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: topology.kubernetes.io/zone
                      operator: Exists
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd-prepare
                  topologyKey: kubernetes.io/hostname
          topologySpreadConstraints:
            - maxSkew: 1
              #topologyKey: kubernetes.io/hostname
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd-prepare
      # OSD daemon limits
      resources:
        limits:
          memory: "4Gi"
        requests:
          cpu: "500m"
          memory: "4Gi"
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: "${ARGOCD_ENV_ceph_volume_size_per_osd}"
          storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
    onlyApplyOSDPlacement: false
  priorityClassNames:
    # If there are multiple nodes available in a failure domain (e.g. zones), the
    # mons and osds can be portable and set the system-cluster-critical priority class.
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
#  disruptionManagement:
#    managePodBudgets: true
#    osdMaintenanceTimeout: 30
#    pgHealthCheckTimeout: 0

# CephObjectStore configurations
#cephObjectStores:
#  - name: ceph-objectstore
#    spec:
#      metadataPool:
#        failureDomain: host
#        replicated:
#          size: 3
#      dataPool:
#        failureDomain: host
#    storageClass:
#      enabled: true
#      name: ceph-bucket
#      reclaimPolicy: Delete
#      volumeBindingMode: "Immediate"
#      parameters:
#        region: "${ARGOCD_ENV_rook_ceph_object_store_region}"
#    ingress:
#      enabled: false
#
# Ceph Block Pools configurations
#cephBlockPools:
#  - name: ceph-blockpool
#    spec:
#      failureDomain: host #host, osd
#      replicated:
#        size: 3
#    storageClass:
#      isDefault: true
#      reclaimPolicy: Delete
#      allowVolumeExpansion: true
#      volumeBindingMode: "Immediate"
#      parameters:
#        csi.storage.k8s.io/fstype: ext4