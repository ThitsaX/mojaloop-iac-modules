---
apiVersion: v1
kind: Namespace
metadata:
  name: ${ARGOCD_ENV_capi_cluster_namespace}
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
    argoCDChart: enabled
  name: ${ARGOCD_ENV_capi_cluster_name}
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
spec:
  controlPlaneEndpoint:
    host: ${ARGOCD_ENV_capi_cluster_kubeapi_host}
    port: ${ARGOCD_ENV_capi_cluster_kubeapi_port}
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: ${ARGOCD_ENV_capi_cluster_name}
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: ProxmoxCluster
    name: ${ARGOCD_ENV_capi_cluster_name}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ProxmoxCluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
  name: ${ARGOCD_ENV_capi_cluster_name}
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
spec:
  controlPlaneEndpoint:
    host: ${ARGOCD_ENV_capi_cluster_kubeapi_host}
    port: ${ARGOCD_ENV_capi_cluster_kubeapi_port}
  serverRef:
    endpoint: "https://${ARGOCD_ENV_capi_cluster_proxmox_url}/api2/json"
    secretRef:
      name: ${ARGOCD_ENV_capi_cluster_name}
  storage:
    name: ${ARGOCD_ENV_capi_cluster_name}
    path: ""
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: ${ARGOCD_ENV_capi_cluster_name}
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
      controllerManager:
        extraArgs:
          cloud-provider: external
      networking:
        dnsDomain: cluster.local
        serviceSubnet: 10.96.0.0/16
        podSubnet: 10.244.0.0/16
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
    preKubeadmCommands:
      - echo "KUBELET_EXTRA_ARGS=--node-ip=$(ip addr show eth0 | grep "inet\b" | awk '{print $2}' | cut -d/ -f1)" >> /etc/default/kubelet
    postKubeadmCommands:
      - "curl -L https://dl.k8s.io/release/v${ARGOCD_ENV_capi_cluster_kube_version}/bin/linux/amd64/kubectl -o /usr/local/bin/kubectl"
      - "chmod +x /usr/local/bin/kubectl"
      - "reboot now"
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: ProxmoxMachineTemplate
      name: "${ARGOCD_ENV_capi_cluster_name}-controlplane"
  #hardcoded to 1 for now
  replicas: 1
  version: "v${ARGOCD_ENV_capi_cluster_kube_version}"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ProxmoxMachineTemplate
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
  name: "${ARGOCD_ENV_capi_cluster_name}-controlplane"
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
spec:
  template:
    spec:
      cloudInit:
        user:
          packages:
            - socat
            - conntrack
          writeFiles:
            - path: /etc/modules-load.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: overlay\nbr_netfilter
            - path: /etc/sysctl.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: |
                net.bridge.bridge-nf-call-iptables  = 1
                net.bridge.bridge-nf-call-ip6tables = 1
                net.ipv4.ip_forward                 = 1
          ssh_authorized_keys:
            - ${ARGOCD_ENV_capi_cluster_proxmox_host_sshkey}
          user: ubuntu
          password: ${ARGOCD_ENV_capi_cluster_proxmox_host_password}
          chpasswd: { expire: "false" }
          manage_etc_hosts: true
          runCmd:
            - "modprobe overlay"
            - "modprobe br_netfilter"
            - "sysctl --system"
            - "mkdir -p /usr/local/bin"
            - curl -L "https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz" | tar Cxvz "/usr/local"
            - curl -L "https://raw.githubusercontent.com/containerd/containerd/main/containerd.service" -o /etc/systemd/system/containerd.service
            - "mkdir -p /etc/containerd"
            - "containerd config default > /etc/containerd/config.toml"
            - "sed 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml -i"
            - "systemctl daemon-reload"
            - "systemctl enable --now containerd"
            - "mkdir -p /usr/local/sbin"
            - curl -L "https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64" -o /usr/local/sbin/runc
            - "chmod 755 /usr/local/sbin/runc"
            - "mkdir -p /opt/cni/bin"
            - curl -L "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz" | tar -C "/opt/cni/bin" -xz
            - curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.27.0/crictl-v1.27.0-linux-amd64.tar.gz" | tar -C "/usr/local/bin" -xz
            - curl -L --remote-name-all https://dl.k8s.io/release/v${ARGOCD_ENV_capi_cluster_kube_version}/bin/linux/amd64/kubeadm -o /usr/local/bin/kubeadm
            - chmod +x /usr/local/bin/kubeadm
            - curl -L --remote-name-all https://dl.k8s.io/release/v${ARGOCD_ENV_capi_cluster_kube_version}/bin/linux/amd64/kubelet -o /usr/local/bin/kubelet
            - chmod +x /usr/local/bin/kubelet
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service
            - mkdir -p /etc/systemd/system/kubelet.service.d
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            - "systemctl enable kubelet.service"
      hardware:
        cpu: ${ARGOCD_ENV_capi_cluster_proxmox_control_plane_cpu}
        memory: ${ARGOCD_ENV_capi_cluster_proxmox_control_plane_memory}
        cpuType: host
        networkDevice:
          bridge: ${ARGOCD_ENV_capi_cluster_proxmox_host_net_interface}
          firewall: true
          model: virtio
          tag: ${ARGOCD_ENV_capi_cluster_proxmox_host_vlan}
        rootDisk: "50G"
      network:
        nameServer: ${ARGOCD_ENV_capi_cluster_network_gateway}
        ipConfig:
          ip: "${ARGOCD_ENV_capi_cluster_kubeapi_host}/${ARGOCD_ENV_capi_cluster_network_subnet}"
          gateway: ${ARGOCD_ENV_capi_cluster_network_gateway}
      storage: ${ARGOCD_ENV_capi_cluster_storage_name}
      image:
        checksum: c5eed826009c9f671bc5f7c9d5d63861aa2afe91aeff1c0d3a4cb5b28b2e35d6
        checksumType: sha256
        url: https://cloud-images.ubuntu.com/releases/jammy/release-20230914/ubuntu-22.04-server-cloudimg-amd64-disk-kvm.img
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
  name: "${ARGOCD_ENV_capi_cluster_name}-md-0"
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
spec:
  clusterName: ${ARGOCD_ENV_capi_cluster_name}
  replicas: ${ARGOCD_ENV_capi_cluster_proxmox_worker_replicas}
  selector:
    matchLabels: {}
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${ARGOCD_ENV_capi_cluster_name}-md-0"
      clusterName: ${ARGOCD_ENV_capi_cluster_name}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: ProxmoxMachineTemplate
        name: "${ARGOCD_ENV_capi_cluster_name}-md-0"
      version: "v${ARGOCD_ENV_capi_cluster_kube_version}"
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${ARGOCD_ENV_capi_cluster_name}-md-0"
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
      preKubeadmCommands:
        - echo "KUBELET_EXTRA_ARGS=--node-ip=$(ip addr show eth0 | grep "inet\b" | awk '{print $2}' | cut -d/ -f1)" >> /etc/default/kubelet
      postKubeadmCommands:
        - "curl -L https://dl.k8s.io/release/v${ARGOCD_ENV_capi_cluster_kube_version}/bin/linux/amd64/kubectl -o /usr/local/bin/kubectl"
        - "chmod +x /usr/local/bin/kubectl"
        - "reboot now"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ProxmoxMachineTemplate
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
  name: "${ARGOCD_ENV_capi_cluster_name}-md-0"
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
spec:
  template:
    spec:
      storage: ${ARGOCD_ENV_capi_cluster_storage_name}
      hardware:
        cpu: ${ARGOCD_ENV_capi_cluster_proxmox_worker_cpu}
        memory: ${ARGOCD_ENV_capi_cluster_proxmox_worker_memory}
        cpuType: host
        networkDevice:
          bridge: ${ARGOCD_ENV_capi_cluster_proxmox_host_net_interface}
          firewall: true
          model: virtio
          tag: ${ARGOCD_ENV_capi_cluster_proxmox_host_vlan}
        rootDisk: "50G"
        extraDisks:
          - size: "100G"
            storage: ${ARGOCD_ENV_capi_cluster_storage_name}
      network:
        ipConfig:
          ip: dhcp
      cloudInit:
        user:
          packages:
            - socat
            - conntrack
          writeFiles:
            - path: /etc/modules-load.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: overlay\nbr_netfilter
            - path: /etc/sysctl.d/k8s.conf
              owner: root:root
              permissions: "0640"
              content: |
                net.bridge.bridge-nf-call-iptables  = 1
                net.bridge.bridge-nf-call-ip6tables = 1
                net.ipv4.ip_forward                 = 1
          ssh_authorized_keys:
            - ${ARGOCD_ENV_capi_cluster_proxmox_host_sshkey}
          user: ubuntu
          password: ${ARGOCD_ENV_capi_cluster_proxmox_host_password}
          chpasswd: { expire: "false" }
          manage_etc_hosts: true
          runCmd:
            - "modprobe overlay"
            - "modprobe br_netfilter"
            - "sysctl --system"
            - "mkdir -p /usr/local/bin"
            - curl -L "https://github.com/containerd/containerd/releases/download/v1.7.2/containerd-1.7.2-linux-amd64.tar.gz" | tar Cxvz "/usr/local"
            - curl -L "https://raw.githubusercontent.com/containerd/containerd/main/containerd.service" -o /etc/systemd/system/containerd.service
            - "mkdir -p /etc/containerd"
            - "containerd config default > /etc/containerd/config.toml"
            - "sed 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml -i"
            - "systemctl daemon-reload"
            - "systemctl enable --now containerd"
            - "mkdir -p /usr/local/sbin"
            - curl -L "https://github.com/opencontainers/runc/releases/download/v1.1.7/runc.amd64" -o /usr/local/sbin/runc
            - "chmod 755 /usr/local/sbin/runc"
            - "mkdir -p /opt/cni/bin"
            - curl -L "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz" | tar -C "/opt/cni/bin" -xz
            - curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.27.0/crictl-v1.27.0-linux-amd64.tar.gz" | tar -C "/usr/local/bin" -xz
            - curl -L --remote-name-all https://dl.k8s.io/release/v${ARGOCD_ENV_capi_cluster_kube_version}/bin/linux/amd64/kubeadm -o /usr/local/bin/kubeadm
            - chmod +x /usr/local/bin/kubeadm
            - curl -L --remote-name-all https://dl.k8s.io/release/v${ARGOCD_ENV_capi_cluster_kube_version}/bin/linux/amd64/kubelet -o /usr/local/bin/kubelet
            - chmod +x /usr/local/bin/kubelet
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service
            - mkdir -p /etc/systemd/system/kubelet.service.d
            - curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.15.1/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            - "systemctl enable kubelet.service"
      image:
        checksum: c5eed826009c9f671bc5f7c9d5d63861aa2afe91aeff1c0d3a4cb5b28b2e35d6
        checksumType: sha256
        url: https://cloud-images.ubuntu.com/releases/jammy/release-20230914/ubuntu-22.04-server-cloudimg-amd64-disk-kvm.img
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
  name: ${ARGOCD_ENV_capi_cluster_name}
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
stringData:
  PROXMOX_PASSWORD: ${ARGOCD_ENV_capi_cluster_proxmox_password}
  PROXMOX_SECRET: ""
  PROXMOX_TOKENID: ""
  PROXMOX_USER: ${ARGOCD_ENV_capi_cluster_proxmox_user}
type: Opaque
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
  name: "${ARGOCD_ENV_capi_cluster_name}-crs-0"
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${ARGOCD_ENV_capi_cluster_name}
  resources:
    - kind: ConfigMap
      name: cloud-controller-manager
  strategy: Reconcile
---
apiVersion: v1
data:
  cloud-controller-manager.yaml: |
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: proxmox-cloud-controller-manager
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: system:proxmox-cloud-controller-manager
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
      name: proxmox-cloud-controller-manager
      namespace: kube-system
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        k8s-app: cloud-controller-manager
      name: cloud-controller-manager
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          k8s-app: cloud-controller-manager
      template:
        metadata:
          labels:
            k8s-app: cloud-controller-manager
        spec:
          serviceAccountName: proxmox-cloud-controller-manager
          containers:
          - name: cloud-controller-manager
            image: ghcr.io/k8s-proxmox/cloud-provider-proxmox:latest
            command:
            - /usr/local/bin/cloud-controller-manager
            - --cloud-provider=proxmox
            - --cloud-config=/etc/proxmox/config.yaml
            - --leader-elect=true
            - --use-service-account-credentials
            - --controllers=cloud-node,cloud-node-lifecycle
            volumeMounts:
              - name: cloud-config
                mountPath: /etc/proxmox
                readOnly: true
            livenessProbe:
              httpGet:
                path: /healthz
                port: 10258
                scheme: HTTPS
              initialDelaySeconds: 20
              periodSeconds: 30
              timeoutSeconds: 5
          volumes:
            - name: cloud-config
              secret:
                secretName: cloud-config
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
            effect: NoSchedule
          - key: node-role.kubernetes.io/master
            operator: Exists
            effect: NoSchedule
          nodeSelector:
            node-role.kubernetes.io/control-plane: ""
    ---
    apiVersion: v1
    kind: Secret
    metadata:
      name: cloud-config
      namespace: kube-system
    stringData:
      config.yaml: |
        proxmox:
          url: https://${ARGOCD_ENV_capi_cluster_proxmox_url}/api2/json
          user: ${ARGOCD_ENV_capi_cluster_proxmox_user}
          password: ${ARGOCD_ENV_capi_cluster_proxmox_password}
          tokenID: ""
          secret: ""
kind: ConfigMap
metadata:
  name: cloud-controller-manager
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
---
apiVersion: addons.cluster.x-k8s.io/v1alpha1
kind: HelmChartProxy
metadata:
  name: argocd
  namespace: ${ARGOCD_ENV_capi_cluster_namespace}
spec:
  clusterSelector:
    matchLabels:
      argoCDChart: enabled
  repoURL: https://argoproj.github.io/argo-helm
  chartName: argo-cd
  namespace: "argocd"
  options:
    waitForJobs: true
    wait: true
    timeout: 5m
    install:
      createNamespace: true
  valuesTemplate: |
    configs:
      cm:
        exec.enabled: "true"
        kustomize.buildOptions: "--enable-helm --helm-kube-version 1.29-0 --load-restrictor LoadRestrictionsNone"
        application.resourceTrackingMethod: annotation

        # DO NOT USE in production, this is only used to improve reconciliation in testing env.
        timeout.reconciliation: "10s"
        resource.customizations: |
          argoproj.io/Application:
            health.lua: |
              hs = {}
              hs.status = "Progressing"
              hs.message = ""
              if obj.status ~= nil then
                local status = obj.status
                if status.conditions ~= nil then
                  for i, condition in ipairs(status.conditions) do
                    if condition.type ~= nil and string.match(condition.type, '.*Error$') then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
                if status.health ~= nil then
                  local health = status.health
                  hs.status = health.status
                  if health.message ~= nil then
                    hs.message = health.message
                  end
                  local syncStatus = (status.sync and status.sync.status or nil)
                  if hs.status == "Healthy" and syncStatus ~= "Synced" then
                    hs.status = "Progressing"
                  end
                end
              end
              return hs
          cert-manager.io/ClusterIssuer:
            health.lua: |
              local hs = {}
              if obj.status ~= nil then
                if obj.status.conditions ~= nil then
                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "Ready" and condition.status == "False" then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                    if condition.type == "Ready" and condition.status == "True" then
                      hs.status = "Healthy"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
              end

              hs.status = "Progressing"
              hs.message = "Initializing ClusterIssuer"
              return hs
          cert-manager.io/Certificate:
            health.lua: |
              local hs = {}
              if obj.status ~= nil then
                if obj.status.conditions ~= nil then

                  -- Always Handle Issuing First to ensure consistent behaviour
                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "Issuing" and condition.status == "True" then
                      hs.status = "Progressing"
                      hs.message = condition.message
                      return hs
                    end
                  end

                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "Ready" and condition.status == "False" then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                    if condition.type == "Ready" and condition.status == "True" then
                      hs.status = "Healthy"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
              end

              hs.status = "Progressing"
              hs.message = "Waiting for certificate"
              return hs
          redhatcop.redhat.io/VaultSecret:
            health.lua: |
              hs = {}
              if obj.status ~= nil then
                if obj.status.conditions ~= nil then
                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "ReconcileSuccessful" and condition.status == "False" then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                    if condition.type == "ReconcileSuccessful" and condition.status == "True" then
                      hs.status = "Healthy"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
              end

              hs.status = "Progressing"
              hs.message = "Waiting for VaultSecret"
              return hs

          redhatcop.redhat.io/KubernetesAuthEngineRole:
            health.lua: |
              hs = {}
              if obj.status ~= nil then
                if obj.status.conditions ~= nil then
                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "ReconcileSuccessful" and condition.status == "False" then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                    if condition.type == "ReconcileSuccessful" and condition.status == "True" then
                      hs.status = "Healthy"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
              end

              hs.status = "Progressing"
              hs.message = "Waiting for KubernetesAuthEngineRole"
              return hs

          ceph.rook.io/CephBlockPool:
            health.lua: |
              hs = {}
              if obj.status ~= nil then
                if obj.status.phase == "Ready" then
                  hs.status = "Healthy"
                  hs.message = "CephBlockPool Ready"
                  return hs
                end
              end
              hs.status = "Progressing"
              hs.message = "Waiting for CephBlockPool"
              return hs

          ceph.rook.io/CephObjectStore:
            health.lua: |
              hs = {}
              if obj.status ~= nil then
                if obj.status.phase == "Ready" then
                  hs.status = "Healthy"
                  hs.message = "CephObjectStore Ready"
                  return hs
                end
              end
              hs.status = "Progressing"
              hs.message = "Waiting for CephObjectStore"
              return hs

          external-secrets.io/ExternalSecret:
            health.lua: |
              local hs = {}
              if obj.status ~= nil then
                if obj.status.conditions ~= nil then
                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "Ready" and condition.status == "False" then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                    if condition.type == "Ready" and condition.status == "True" then
                      hs.status = "Healthy"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
              end
              hs.status = "Progressing"
              hs.message = "Waiting for ExternalSecret"
              return hs
          external-secrets.io/ClusterSecretStore:
            health.lua: |
              local hs = {}
              if obj.status ~= nil then
                if obj.status.conditions ~= nil then
                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "Ready" and condition.status == "False" then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                    if condition.type == "Ready" and condition.status == "True" then
                      hs.status = "Healthy"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
              end
              hs.status = "Progressing"
              hs.message = "Waiting for ClusterSecretStore"
              return hs

          external-secrets.io/SecretStore:
            health.lua: |
              local hs = {}
              if obj.status ~= nil then
                if obj.status.conditions ~= nil then
                  for i, condition in ipairs(obj.status.conditions) do
                    if condition.type == "Ready" and condition.status == "False" then
                      hs.status = "Degraded"
                      hs.message = condition.message
                      return hs
                    end
                    if condition.type == "Ready" and condition.status == "True" then
                      hs.status = "Healthy"
                      hs.message = condition.message
                      return hs
                    end
                  end
                end
              end
              hs.status = "Progressing"
              hs.message = "Waiting for SecretStore"
              return hs

          "*.upbound.io/*":
            health.lua: |
              health_status = {
                status = "Progressing",
                message = "Provisioning ..."
              }

              local function contains (table, val)
                for i, v in ipairs(table) do
                  if v == val then
                    return true
                  end
                end
                return false
              end

              local has_no_status = {
                "ProviderConfig",
                "ProviderConfigUsage"
              }

              if obj.status == nil or next(obj.status) == nil and contains(has_no_status, obj.kind) then
                health_status.status = "Healthy"
                health_status.message = "Resource is up-to-date."
                return health_status
              end

              if obj.status == nil or next(obj.status) == nil or obj.status.conditions == nil then
                if obj.kind == "ProviderConfig" and obj.status.users ~= nil then
                  health_status.status = "Healthy"
                  health_status.message = "Resource is in use."
                  return health_status
                end
                return health_status
              end

              for i, condition in ipairs(obj.status.conditions) do
                if condition.type == "LastAsyncOperation" then
                  if condition.status == "False" then
                    health_status.status = "Degraded"
                    health_status.message = condition.message
                    return health_status
                  end
                end

                if condition.type == "Synced" then
                  if condition.status == "False" then
                    health_status.status = "Degraded"
                    health_status.message = condition.message
                    return health_status
                  end
                end

                if condition.type == "Ready" then
                  if condition.status == "True" then
                    health_status.status = "Healthy"
                    health_status.message = "Resource is up-to-date."
                    return health_status
                  end
                end
              end

              return health_status

          "*.crossplane.io/*":
            health.lua: |
              health_status = {
                status = "Progressing",
                message = "Provisioning ..."
              }

              local function contains (table, val)
                for i, v in ipairs(table) do
                  if v == val then
                    return true
                  end
                end
                return false
              end

              local has_no_status = {
                "Composition",
                "CompositionRevision",
                "DeploymentRuntimeConfig",
                "ControllerConfig",
                "ProviderConfig",
                "ProviderConfigUsage"
              }
              if obj.status == nil or next(obj.status) == nil and contains(has_no_status, obj.kind) then
                  health_status.status = "Healthy"
                  health_status.message = "Resource is up-to-date."
                return health_status
              end

              if obj.status == nil or next(obj.status) == nil or obj.status.conditions == nil then
                if obj.kind == "ProviderConfig" and obj.status.users ~= nil then
                  health_status.status = "Healthy"
                  health_status.message = "Resource is in use."
                  return health_status
                end
                return health_status
              end

              for i, condition in ipairs(obj.status.conditions) do
                if condition.type == "LastAsyncOperation" then
                  if condition.status == "False" then
                    health_status.status = "Degraded"
                    health_status.message = condition.message
                    return health_status
                  end
                end

                if condition.type == "Synced" then
                  if condition.status == "False" then
                    health_status.status = "Degraded"
                    health_status.message = condition.message
                    return health_status
                  end
                end

                if contains({"Ready", "Healthy", "Offered", "Established"}, condition.type) then
                  if condition.status == "True" then
                    health_status.status = "Healthy"
                    health_status.message = "Resource is up-to-date."
                    return health_status
                  end
                end
              end

              return health_status


      params:
        server.insecure: true
        # Mandatory for extensions to work
        server.enable.proxy.extension: "true"
      cmp:
        create: true
        plugins:
          envsubstappofapp:
            init:
              command: ["sh", "-c"]
              args:
                [
                  "kustomize build . --load-restrictor LoadRestrictionsNone -o raw-kustomization.yaml",
                ]
            generate:
              command: ["sh", "-c"]
              args:
                [
                  "envsubst < raw-kustomization.yaml > processed-kustomization.yaml && cp processed-kustomization.yaml /dev/stdout",
                ]
            discover:
              fileName: "kustomization.*"
          envsubst:
            discover:
              fileName: "kustomization.*"
            generate:
              command: ["sh", "-c"]
              args:
                [
                  "for f in *.yaml ; do cat $f | envsubst > $f.sub && mv $f.sub $f ; done && kustomize build . --enable-helm --helm-kube-version 1.29-0 --load-restrictor LoadRestrictionsNone > /dev/stdout",
                ]

    repoServer:
      # resources:
      #   limits:
      #     cpu: 500m
      #     memory: 1.5Gi
      #   requests:
      #     cpu: 250m
      #     memory: 512Mi

      env:
        - name: HELM_CACHE_HOME
          value: /helm-working-dir
        - name: HELM_CONFIG_HOME
          value: /helm-working-dir
        - name: HELM_DATA_HOME
          value: /helm-working-dir

      volumes:
        - name: custom-tools
          emptyDir: {}
        - name: cmp-plugin
          configMap:
            name: argocd-cmp-cm

      initContainers:
        - name: download-tools
          image: golang:1.22.4-alpine3.20
          command: [sh, -c]
          args:
            - apk add git && go install github.com/drone/envsubst/cmd/envsubst@v1.0.3 && mv $GOPATH/bin/envsubst /custom-tools/ && wget https://gist.githubusercontent.com/enriched/11c7d81aa271b258f835620b1aca2e55/raw/2017a4202da30506b1fedc2981afed41e259fe77/in-pod-kubeconfig.sh && chmod +x in-pod-kubeconfig.sh && mv in-pod-kubeconfig.sh /custom-tools/
          volumeMounts:
            - mountPath: /custom-tools
              name: custom-tools

      extraContainers:
        - name: debug-tools
          image: quay.io/argoproj/argocd
          command: [sh, -c]
          args:
            - while true; do echo "running"; sleep 300; done
          volumeMounts:
            - mountPath: /var/run/argocd
              name: var-files
            - mountPath: /home/argocd/cmp-server/plugins
              name: plugins
            - mountPath: /tmp
              name: tmp

            # Important: Mount tools into $PATH
            - name: custom-tools
              subPath: envsubst
              mountPath: /usr/local/bin/envsubst
            - name: custom-tools
              subPath: in-pod-kubeconfig.sh
              mountPath: /usr/local/bin/in-pod-kubeconfig.sh
            - name: helm-working-dir
              mountPath: /helm-working-dir

        - name: envsubstappofapp
          command: [/var/run/argocd/argocd-cmp-server]
          image: quay.io/argoproj/argocd
          args: [--loglevel, debug]
          securityContext:
            runAsNonRoot: true
            runAsUser: 999
          volumeMounts:
            - mountPath: /var/run/argocd
              name: var-files
            - mountPath: /home/argocd/cmp-server/plugins
              name: plugins
            - mountPath: /tmp
              name: tmp

            # Register plugins into sidecar
            - mountPath: /home/argocd/cmp-server/config/plugin.yaml
              subPath: envsubstappofapp.yaml
              name: cmp-plugin

            # Important: Mount tools into $PATH
            - name: custom-tools
              subPath: envsubst
              mountPath: /usr/local/bin/envsubst

        - name: envsubst
          command: [/var/run/argocd/argocd-cmp-server]
          image: quay.io/argoproj/argocd
          args: [--loglevel, debug]
          securityContext:
            runAsNonRoot: true
            runAsUser: 999
          volumeMounts:
            - mountPath: /var/run/argocd
              name: var-files
            - mountPath: /home/argocd/cmp-server/plugins
              name: plugins
            - mountPath: /tmp
              name: tmp

            # Register plugins into sidecar
            - mountPath: /home/argocd/cmp-server/config/plugin.yaml
              subPath: envsubst.yaml
              name: cmp-plugin

            # Important: Mount tools into $PATH
            - name: custom-tools
              subPath: envsubst
              mountPath: /usr/local/bin/envsubst
---
# apiVersion: addons.cluster.x-k8s.io/v1alpha1
# kind: HelmChartProxy
# metadata:
#   name: storage-cluster-root-app
#   namespace: ${ARGOCD_ENV_capi_cluster_namespace}
# spec:
#   clusterSelector:
#     matchLabels:
#       argoCDChart: enabled
#   repoURL: https://argoproj.github.io/argo-helm
#   chartName: argocd-apps
#   namespace: "argocd"
#   options:
#     waitForJobs: true
#     wait: true
#     timeout: 5m
#     install:
#       createNamespace: true
#   valuesTemplate: |
#     applications:
#       storage-cluster-deployer:
#         namespace: argocd
#         finalizers:
#           - resources-finalizer.argocd.argoproj.io
#         project: default
#         syncPolicy:
#           automated:
#             prune: true
#             selfHeal: true
#           retry:
#             limit: 60
#             backoff:
#               duration: 10s
#               maxDuration: 1m0s
#               factor: 2
#           syncOptions:
#             - CreateNamespace=true
#             - PrunePropagationPolicy=foreground
#             - PruneLast=true
#         sources:
#           - repoURL: ${ARGOCD_ENV_sc_root_argocd_repo_url}
#             targetRevision: ${ARGOCD_ENV_sc_root_application_gitrepo_tag}
#             path: gitops/argo-apps/overlays/local/storage-cluster
#             plugin:
#               name: envsubstappofapp
#               env: []

#         destination:
#           server: "https://kubernetes.default.svc"
#           namespace: argocd
